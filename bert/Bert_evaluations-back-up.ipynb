{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"eval_utils\")\n",
    "\n",
    "from eval_utils.seat_v2_and_logprob import convert_tmps_to_sents, get_embeddings_and_probs, add_assos\n",
    "from eval_utils.seat_v1 import get_embeddings, compute_effect_size\n",
    "from eval_utils.evaluating_over_debiasing import add_predicts, get_vios_and_dif\n",
    "from eval_utils.bias_utils.debiasing_effects_util import get_effect_size\n",
    "\n",
    "eval_type = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT_BASE_UNCASED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:19:01,539 : INFO : The debiasing effects evaluation type is test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tmps from ../data/simple_patterns\n",
      "--- 879 sentens to be processes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:19:04,022 : INFO : ****** The model is bert-base-uncased\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2021-09-12 19:19:08,872 : INFO : ******1 The type of used model is <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- get embeddings from TM and TAM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/543 [00:00<?, ?it/s]intial_eval_utils\\bias_utils\\utils_for_diverse_tmps.py:297: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  mask_index = (sents[sent_id] == tokenizer.mask_token_id).nonzero().flatten().tolist()[masked_id]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 543/543 [00:19<00:00, 27.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 336/336 [00:09<00:00, 34.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SEAT: effect size is 1.9427828883830491, p value is 0.0\n",
      "For log probability score: effect size is 1.966382127046125, p value is 0.0\n"
     ]
    }
   ],
   "source": [
    "tmp_path = '../data/simple_patterns'\n",
    "\n",
    "corpus, tmps_type, temp_size = convert_tmps_to_sents(tmp_path, eval_type,\n",
    "                                                 simple = True)\n",
    "sent2emb, model_type = get_embeddings_and_probs(corpus, tmps_type, temp_size, model_type = 'bert-base-uncased')\n",
    "corpus = add_assos(corpus, tmps_type, temp_size, model_type = 'bert-base-uncased', sent2emb = sent2emb)\n",
    "\n",
    "seat_effect_size, seat_p, log_effect_size, log_p = get_effect_size(corpus)\n",
    "print(\"For SEAT: effect size is {}, p value is {}\".format(seat_effect_size, seat_p))\n",
    "print(\"For log probability score: effect size is {}, p value is {}\".format(log_effect_size, log_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 234 sentens to be processes ---\n",
      "We will use the GPU: GeForce GTX 1650\n",
      "load model from bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len evaluation: 128\n",
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 234/234 [00:06<00:00, 35.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For standard SEAT: effect size is 1.7004990198053145, p value is 0.0\n"
     ]
    }
   ],
   "source": [
    "if eval_type == 'val':\n",
    "    ori_seat_corpus_path = '../data/val_new_ori_seat_data.json'\n",
    "elif eval_type == 'test':\n",
    "    ori_seat_corpus_path = '../data/test_new_ori_seat_data.json'\n",
    "else:\n",
    "    assert eval_type == 'whole'\n",
    "    ori_seat_corpus_path = '../data/new_ori_seat_data.json'\n",
    "\n",
    "with open(ori_seat_corpus_path, 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "sent2emb = get_embeddings(test, model_type = 'bert-base-uncased')\n",
    "\n",
    "seat_ez, p = compute_effect_size(test, sent2emb = sent2emb)\n",
    "\n",
    "print(\"For standard SEAT: effect size is {}, p value is {}\".format(seat_ez, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2610/2610 [03:03<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On UAL_Gender\n",
      "{'error_num': 128, 'error_rate': 0.04904214559386973, 'IA': 2.51870070982319}\n"
     ]
    }
   ],
   "source": [
    "data_without_preds = pd.read_csv('../data/2610_sents_for_evaluating_gender_loss.tsv', sep='\\t', index_col = 0)\n",
    "data = add_predicts(data_without_preds, model_type = 'bert-base-uncased')\n",
    "\n",
    "violates, difs = get_vios_and_dif(data)\n",
    "\n",
    "over_debias = {}\n",
    "\n",
    "error_num = len(violates)\n",
    "over_debias['error_num'] = error_num\n",
    "\n",
    "error_rate = len(violates)/len(data)\n",
    "over_debias['error_rate'] = error_rate\n",
    "\n",
    "IA = sum(difs) / len(difs)\n",
    "over_debias['IA'] = IA\n",
    "\n",
    "print(\"On UAL_Gender\")\n",
    "print(over_debias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Sent_debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:32:34,785 : INFO : The debiasing effects evaluation type is test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tmps from ../data/simple_patterns\n",
      "* load gender dir from  bert_save_embeddings/base_sent/gender_dir_0.05\n",
      "--- 879 sentens to be processes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:32:36,910 : INFO : ****** The model is bert-base-uncased\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2021-09-12 19:32:41,635 : INFO : ******1 The type of used model is <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- get embeddings from TM and TAM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/543 [00:00<?, ?it/s]intial_eval_utils\\bias_utils\\utils_for_diverse_tmps.py:297: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  mask_index = (sents[sent_id] == tokenizer.mask_token_id).nonzero().flatten().tolist()[masked_id]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 543/543 [00:39<00:00, 13.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debiasing by sent_debias1...\n",
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 336/336 [00:09<00:00, 34.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debiasing by sent_debias2...\n",
      "For SEAT: effect size is 1.3632698257145888, p value is 0.00377\n",
      "For log probability score: effect size is 1.0976487652451454, p value is 0.02352\n"
     ]
    }
   ],
   "source": [
    "tmp_path = '../data/simple_patterns'\n",
    "\n",
    "corpus, tmps_type, temp_size = convert_tmps_to_sents(tmp_path, eval_type,\n",
    "                                                 simple = True)\n",
    "sent2emb, model_type = get_embeddings_and_probs(corpus, tmps_type, temp_size, model_type = 'bert-base-uncased', subspace_path = 'bert_save_embeddings/base_sent/gender_dir_0.05')\n",
    "corpus = add_assos(corpus, tmps_type, temp_size, model_type = 'bert-base-uncased', sent2emb = sent2emb)\n",
    "\n",
    "seat_effect_size, seat_p, log_effect_size, log_p = get_effect_size(corpus)\n",
    "print(\"For SEAT: effect size is {}, p value is {}\".format(seat_effect_size, seat_p))\n",
    "print(\"For log probability score: effect size is {}, p value is {}\".format(log_effect_size, log_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* load gender dir from  bert_save_embeddings/base_sent/gender_dir_0.05\n",
      "--- 234 sentens to be processes ---\n",
      "We will use the GPU: GeForce GTX 1650\n",
      "load model from bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len evaluation: 128\n",
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 234/234 [00:06<00:00, 33.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debiasing by sent_debias...\n",
      "For standard SEAT: effect size is -0.09647898803104707, p value is 0.68141\n"
     ]
    }
   ],
   "source": [
    "if eval_type == 'val':\n",
    "    ori_seat_corpus_path = '../data/val_new_ori_seat_data.json'\n",
    "elif eval_type == 'test':\n",
    "    ori_seat_corpus_path = '../data/test_new_ori_seat_data.json'\n",
    "else:\n",
    "    assert eval_type == 'whole'\n",
    "    ori_seat_corpus_path = '../data/new_ori_seat_data.json'\n",
    "\n",
    "with open(ori_seat_corpus_path, 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "sent2emb = get_embeddings(test, model_type = 'bert-base-uncased', subspace_path = 'bert_save_embeddings/base_sent/gender_dir_0.05')\n",
    "\n",
    "seat_ez, p = compute_effect_size(test, sent2emb = sent2emb)\n",
    "\n",
    "print(\"For standard SEAT: effect size is {}, p value is {}\".format(seat_ez, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* load gender dir from  bert_save_embeddings/base_sent/gender_dir_0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                         | 0/2610 [00:00<?, ?it/s]C:\\Hello_Mulin\\My_study-base\\experiments\\phase2\\Unimelb_fairness_project_phase2\\bert\\initial_evaluation\\intial_eval_utils\\evaluating_over_debiasing.py:87: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  TM_mask_pos = (TM_input_b[b] == tokenizer.mask_token_id).nonzero().flatten().tolist()[0]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2610/2610 [06:35<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On UAL_Gender\n",
      "{'error_num': 842, 'error_rate': 0.3226053639846743, 'IA': 0.09752957079234739}\n"
     ]
    }
   ],
   "source": [
    "data_without_preds = pd.read_csv('../data/2610_sents_for_evaluating_gender_loss.tsv', sep='\\t', index_col = 0)\n",
    "data = add_predicts(data_without_preds, model_type = 'bert-base-uncased', subspace_path = 'bert_save_embeddings/base_sent/gender_dir_0.05')\n",
    "\n",
    "violates, difs = get_vios_and_dif(data)\n",
    "\n",
    "over_debias = {}\n",
    "\n",
    "error_num = len(violates)\n",
    "over_debias['error_num'] = error_num\n",
    "\n",
    "error_rate = len(violates)/len(data)\n",
    "over_debias['error_rate'] = error_rate\n",
    "\n",
    "IA = sum(difs) / len(difs)\n",
    "over_debias['IA'] = IA\n",
    "\n",
    "print(\"On UAL_Gender\")\n",
    "print(over_debias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT_LARGE_UNCASED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:43:51,541 : INFO : The debiasing effects evaluation type is test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tmps from ../data/simple_patterns\n",
      "--- 879 sentens to be processes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:43:52,857 : INFO : ****** The model is bert-large-uncased\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2021-09-12 19:44:04,346 : INFO : ******1 The type of used model is <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- get embeddings from TM and TAM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 543/543 [00:31<00:00, 17.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 336/336 [00:17<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SEAT: effect size is 1.4932444391633548, p value is 0.00154\n",
      "For log probability score: effect size is 1.9723877121303433, p value is 0.0\n"
     ]
    }
   ],
   "source": [
    "tmp_path = '../data/simple_patterns'\n",
    "\n",
    "corpus, tmps_type, temp_size = convert_tmps_to_sents(tmp_path, eval_type,\n",
    "                                                 simple = True)\n",
    "sent2emb, model_type = get_embeddings_and_probs(corpus, tmps_type, temp_size, model_type = 'bert-large-uncased')\n",
    "corpus = add_assos(corpus, tmps_type, temp_size, model_type = 'bert-large-uncased', sent2emb = sent2emb)\n",
    "\n",
    "seat_effect_size, seat_p, log_effect_size, log_p = get_effect_size(corpus)\n",
    "print(\"For SEAT: effect size is {}, p value is {}\".format(seat_effect_size, seat_p))\n",
    "print(\"For log probability score: effect size is {}, p value is {}\".format(log_effect_size, log_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 234 sentens to be processes ---\n",
      "We will use the GPU: GeForce GTX 1650\n",
      "load model from bert-large-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len evaluation: 128\n",
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 234/234 [00:12<00:00, 19.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For standard SEAT: effect size is 0.3350514372963267, p value is 0.15412\n"
     ]
    }
   ],
   "source": [
    "if eval_type == 'val':\n",
    "    ori_seat_corpus_path = '../data/val_new_ori_seat_data.json'\n",
    "elif eval_type == 'test':\n",
    "    ori_seat_corpus_path = '../data/test_new_ori_seat_data.json'\n",
    "else:\n",
    "    assert eval_type == 'whole'\n",
    "    ori_seat_corpus_path = '../data/new_ori_seat_data.json'\n",
    "\n",
    "with open(ori_seat_corpus_path, 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "sent2emb = get_embeddings(test, model_type = 'bert-large-uncased')\n",
    "\n",
    "seat_ez, p = compute_effect_size(test, sent2emb = sent2emb)\n",
    "\n",
    "print(\"For standard SEAT: effect size is {}, p value is {}\".format(seat_ez, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2610/2610 [05:06<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On UAL_Gender\n",
      "{'error_num': 36, 'error_rate': 0.013793103448275862, 'IA': 3.2428213802991674}\n"
     ]
    }
   ],
   "source": [
    "data_without_preds = pd.read_csv('../data/2610_sents_for_evaluating_gender_loss.tsv', sep='\\t', index_col = 0)\n",
    "data = add_predicts(data_without_preds, model_type = 'bert-large-uncased')\n",
    "\n",
    "violates, difs = get_vios_and_dif(data)\n",
    "\n",
    "over_debias = {}\n",
    "\n",
    "error_num = len(violates)\n",
    "over_debias['error_num'] = error_num\n",
    "\n",
    "error_rate = len(violates)/len(data)\n",
    "over_debias['error_rate'] = error_rate\n",
    "\n",
    "IA = sum(difs) / len(difs)\n",
    "over_debias['IA'] = IA\n",
    "\n",
    "print(\"On UAL_Gender\")\n",
    "print(over_debias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Sent_debias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:51:33,306 : INFO : The debiasing effects evaluation type is test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tmps from ../data/simple_patterns\n",
      "* load gender dir from  bert_save_embeddings/large_sent/gender_dir_0.05\n",
      "--- 879 sentens to be processes ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-12 19:51:34,566 : INFO : ****** The model is bert-large-uncased\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2021-09-12 19:51:44,546 : INFO : ******1 The type of used model is <class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- get embeddings from TM and TAM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 543/543 [01:07<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debiasing by sent_debias1...\n",
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 336/336 [00:18<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debiasing by sent_debias2...\n",
      "For SEAT: effect size is 0.3249787582763519, p value is 0.53781\n",
      "For log probability score: effect size is 1.2563850790167035, p value is 0.00571\n"
     ]
    }
   ],
   "source": [
    "tmp_path = '../data/simple_patterns'\n",
    "\n",
    "corpus, tmps_type, temp_size = convert_tmps_to_sents(tmp_path, eval_type,\n",
    "                                                 simple = True)\n",
    "sent2emb, model_type = get_embeddings_and_probs(corpus, tmps_type, temp_size, model_type = 'bert-large-uncased', subspace_path = 'bert_save_embeddings/large_sent/gender_dir_0.05')\n",
    "corpus = add_assos(corpus, tmps_type, temp_size, model_type = 'bert-large-uncased', sent2emb = sent2emb)\n",
    "\n",
    "seat_effect_size, seat_p, log_effect_size, log_p = get_effect_size(corpus)\n",
    "print(\"For SEAT: effect size is {}, p value is {}\".format(seat_effect_size, seat_p))\n",
    "print(\"For log probability score: effect size is {}, p value is {}\".format(log_effect_size, log_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* load gender dir from  bert_save_embeddings/large_sent/gender_dir_0.05\n",
      "--- 234 sentens to be processes ---\n",
      "We will use the GPU: GeForce GTX 1650\n",
      "load model from bert-large-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len evaluation: 128\n",
      "--- get embeddings from AM---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 234/234 [00:12<00:00, 19.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debiasing by sent_debias...\n",
      "For standard SEAT: effect size is 0.025817217813447542, p value is 0.91498\n"
     ]
    }
   ],
   "source": [
    "if eval_type == 'val':\n",
    "    ori_seat_corpus_path = '../data/val_new_ori_seat_data.json'\n",
    "elif eval_type == 'test':\n",
    "    ori_seat_corpus_path = '../data/test_new_ori_seat_data.json'\n",
    "else:\n",
    "    assert eval_type == 'whole'\n",
    "    ori_seat_corpus_path = '../data/new_ori_seat_data.json'\n",
    "\n",
    "with open(ori_seat_corpus_path, 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "sent2emb = get_embeddings(test, model_type = 'bert-large-uncased', subspace_path = 'bert_save_embeddings/large_sent/gender_dir_0.05')\n",
    "\n",
    "seat_ez, p = compute_effect_size(test, sent2emb = sent2emb)\n",
    "\n",
    "print(\"For standard SEAT: effect size is {}, p value is {}\".format(seat_ez, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* load gender dir from  bert_save_embeddings/large_sent/gender_dir_0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                                                         | 0/2610 [00:00<?, ?it/s]C:\\Hello_Mulin\\My_study-base\\experiments\\phase2\\Unimelb_fairness_project_phase2\\bert\\initial_evaluation\\intial_eval_utils\\evaluating_over_debiasing.py:87: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  TM_mask_pos = (TM_input_b[b] == tokenizer.mask_token_id).nonzero().flatten().tolist()[0]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2610/2610 [10:51<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On UAL_Gender\n",
      "{'error_num': 662, 'error_rate': 0.25363984674329504, 'IA': 0.20715517394418945}\n"
     ]
    }
   ],
   "source": [
    "data_without_preds = pd.read_csv('../data/2610_sents_for_evaluating_gender_loss.tsv', sep='\\t', index_col = 0)\n",
    "data = add_predicts(data_without_preds, model_type = 'bert-large-uncased', subspace_path = 'bert_save_embeddings/large_sent/gender_dir_0.05')\n",
    "\n",
    "violates, difs = get_vios_and_dif(data)\n",
    "\n",
    "over_debias = {}\n",
    "\n",
    "error_num = len(violates)\n",
    "over_debias['error_num'] = error_num\n",
    "\n",
    "error_rate = len(violates)/len(data)\n",
    "over_debias['error_rate'] = error_rate\n",
    "\n",
    "IA = sum(difs) / len(difs)\n",
    "over_debias['IA'] = IA\n",
    "\n",
    "print(\"On UAL_Gender\")\n",
    "print(over_debias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
